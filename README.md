# deep-fMC_paper

This repository contains experiments and analysis related to the publication in https://www.biorxiv.org/content/10.1101/2025.01.29.635381v1

Data files for input and output from exp files can be found at https://osf.io/tvu52/

## Repository structure

### 1. `data`
- **Purpose**: Holds input data files for experiments, as well as outputs generated from the experiments (available at https://osf.io/tvu52/)

### 2. `exp`
- **Purpose**: Contains experiment notebooks, scripts, and their direct outputs. Each subfolder (e.g., `exp01`, `exp02`, etc.) corresponds to a set of related analyses, ordered when possible.

### 3. `figures`
- **Purpose**: Stores figures generated by each experiment, organized by experiment folder (e.g., `exp01`, `exp02`, etc.). Inside each experiment folder, figures may be further grouped by notebook number (e.g., `1_figures`, `3_figures`, `4_figures`).

### 4. `scripts`
- **Purpose**: Contains utility scripts or modules that provide functions used across experiments.

## Creating conda enviroment to run the notebooks under `exp`
Run the following bash code in the terminal window to install the packages required to run the Jupyter notebooks under `exp` in a [conda](https://www.anaconda.com/download/success) enviroment called deep-fMC_paper
```bash
conda create -n deep-fMC_paper python=3.10.11 "numpy<2" -y && \
conda activate deep-fMC_paper && \
pip install tensorflow==2.10.0 fastcluster && \
conda install -c conda-forge cudatoolkit=11.8 cudnn=8.8.0.121 -y && \
conda install -c conda-forge "numpy<2" pandas scanpy matplotlib scikit-learn -y && \
conda install -n deep-fMC_paper ipykernel --update-deps --force-reinstall -y && \
conda install -c conda-forge geopandas -y
```

## Running the trained model to call functional archetypes in your own dataset

### Environment set-up
We performed deep archetypal analysis using the [scAAnet](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1010025) framework: https://github.com/AprilYuge/scAAnet/blob/main/

The trained model was developed in a conda environment with the following specifications:
- Python 3.10.11
- TensorFlow 2.10.0
- CUDA 11.8

We recommend cloning the scAAnet repository rather than installing it via pip to ensure compatibility with its dependencies.

To create the conda environment and install the necessary dependencies, use the following command (skip this step if you already created the enviroment described above):

```bash
conda create -n scAAnet python=3.10.11 -y && conda activate scAAnet && pip install tensorflow==2.10.0 && conda install -c conda-forge cudatoolkit=11.8 cudnn=8.8.0.121 -y
```

If using local machine to clone scAAnet repo

```bash
git clone https://github.com/AprilYuge/scAAnet.git 
```

### Preparing Your Data
To assign archetypes using the model trained on healthy gut microbiome samples, you must have pathway relative abundances generated with HUMAnN3 and formatted as an [AnnData object](https://anndata.readthedocs.io/en/latest/tutorials/notebooks/getting-started.html)

Ensure that the data/trained_model/features_model_trained_on.csv file (which contains the pathways used in training, available at https://osf.io/tvu52/) matches the pathways in your input data.


### Load and use the trained model
Below is a sample code snippet (from `exp/exp04_exp04_preprocess_and_deepaa_with_disease/3_deepaa_model_disease_analysis.ipynb`) showing how to load and use the trained model. Make sure to replace any paths and variable names with your actual locations and data.

We modified specific functions from the original scAAnet implementation in order to enable the saving and loading of the trained model as an .h5 file. These modifications were made to the network.py file from the scAAnet repository.
The modified functions are imported as follows:
```python
from modified_network import ZFixedLayer, DispLayer, DispAct, create_z_fixed, ColwiseMultLayer
```
The file modified_network.py is an adapted version of the network.py file from the original scAAnet repository. These changes allow the trained model, encoder, and decoder to be saved and later reloaded for downstream analyses, which is not supported in the original scAAnet implementation.

If you wish to reproduce this workflow, ensure that the modified_network.py file is placed in the appropriate directory (e.g., ../../scripts) and that it is correctly referenced in your code.

```python
import numpy as np
from tensorflow.keras.models import load_model
#if you cloned the scAAnet repo and replaced the network file with modified_network.py use:
#sys.path.append('../../scAAnet') 
#from network import ZFixedLayer, DispLayer, DispAct, create_z_fixed, ColwiseMultLayer
#else use: 
#modified_network.py is a modified version of the top section of https://github.com/AprilYuge/scAAnet/blob/main/scAAnet/network.py to allow model saving and loading as .h5 file 
sys.path.append('../../scripts')
from modified_network import ZFixedLayer, DispLayer, DispAct, create_z_fixed, ColwiseMultLayer 

# features_model_trained_on.csv contains the pathways that the model needs and was trained on
# concat_ad is an anndata object of your samples 
if isinstance(concat_ad, ad.AnnData):
    if sp.issparse(concat_ad.X):
        concat_ad = concat_ad.X.todense()
    else:
        concat_ad = concat_ad.X
count_samples = np.asmatrix(concat_ad).astype('float32')

# Convert counts to TPM by dividing by the sum of counts per sample
TPM_samples = count_samples / count_samples.sum(axis=1)

# Calculate the library size (sum of all counts per sample)
lib_size_samples = count_samples.sum(axis=1)

model_path = "../../data/trained_model"
# Load the trained model to get reconstruction of count matrix
model = load_model(
    f"{model_path}/model.h5",
    custom_objects={
        "ZFixedLayer": ZFixedLayer,
        "DispLayer": DispLayer,
        "DispAct": DispAct,
        "create_z_fixed": create_z_fixed
    }
)
recon = model.predict({"nor_count": TPM_samples, "lib_size": lib_size_samples})

# Load and run the encoder to get samples' archetype usage 
encoder = load_model(
    f"{model_path}/trained_encoder.h5",
    custom_objects={
        "ZFixedLayer": ZFixedLayer,
        "DispLayer": DispLayer,
        "DispAct": DispAct,
        "create_z_fixed": create_z_fixed
    }
)
usage = encoder.predict({"nor_count": TPM_samples, "lib_size": TPM_samples})

# Load and run the decoder to get the spectra of the archetypes 
decoder = load_model(
    f"{model_path}/trained_decoder.h5",
    custom_objects={
        "ZFixedLayer": ZFixedLayer,
        "DispLayer": DispLayer,
        "DispAct": DispAct,
        "create_z_fixed": create_z_fixed
    }
)
# Retrieve the latent representation from the model
z_fixed_weights = model.get_layer("z_fixed").get_weights()[0]
spectra = decoder.predict(z_fixed_weights)
```

### Output details

- recon: The reconstructed count matrix based on the input data.
- usage: Archetype usage, which quantifies how much each archetype contributes to each sample.
- spectra: The latent representation or spectra of the archetypes, representing their key features.

